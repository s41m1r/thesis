
\todo[inline]{Describe the problem of discovering the time perspective}

A relevant characteristic of development processes is the time it takes to perform certain key tasks of the process. For example, in order to monitor whether a new version of a software will be successfully released within a given deadline, a project manager needs to know what is the progress with respect to the estimated delivery. All information that concerns the timing of work is referred to as the \emph{time perspective} the process. 
 

To manage the time perspective, complex software development projects are often planned by the use of a Gantt chart. This chart consists of activities and work packages. Each work-package groups together similar types of activities. For example, all activities done in order to write the documentation of the project, will pertain to the work-package documentation.bpm2015background This inclusion also defines a hierarchy on the tasks. For example, the code of a feature \emph{A} may be located in a file \emph{F}. In turn, file \emph{F} may be in a folder \emph{D} which itself may be included on a super-folder \emph{Code}. This gives the hierarchy \emph{Code/D/F/A}. A typical way in which such hierarchy is organized in a file system is exactly through folder-in-folder and file-in-folder inclusions. 

In order to perform the tasks of process monitoring, we need to gather meaningful time information from event logs. Resulting insights must be presented as a Gantt chart. This allows for comparing the actual Gantt chart of stemming from the data to planned one. Thus, possible deviations in terms of timing become easy to spot. Examples of deviations are related to the following questions: \emph{when did an activity start?} \emph{When did it finish?} \emph{How much work was performed in an time interval?} \emph{Are there any activities that are running late?} \emph{When did a bottleneck occur?} 
%- The minimum unit of work is a change to a file

In line with the above description, we define the following requirements of a solution. 
\begin{itemize}
	\item[\textbf{Aggregate events.}] The developed artifact must be able to aggregate low-level file changes onto work-activities. 
	\item[\textbf{Organize activities into work-packages.}] The identified activities must be organized hierarchically organized within a work-package hierarchy. 
	\item[\textbf{Compute KPIs.}] The developed artifact must provide quantitative measures of the characteristics of a work-package.
\end{itemize}

\section{Related work} 
\label{sec:bpm2015related}

The problem described has been addressed in the literature from different perspectives. The first category of related work tackles the problem by transforming it into a process mining problem. Consequently, approaches have been developed to preprocess VCS data such that process mining techniques can be applied, and hence, a business process can be derived from the log data.
In this group, Kindler et al. \citep{Kindler2006,kindler2006incremental} developed an algorithm for extracting software processes that are mapped to Petri Nets. Activities, which are not explicit in the logs, are discovered from their input and output artifacts. However, strong assumptions are made on the filenames as well as on the software process lifecycle. %(always design, code, review, testres). Activities (which are not explicit in the logs, like in our case) are discovered from their input and output artifacts. Here defined as triples $<I,O,R>$ where I=input, O=Output, R=resouce who perfomed it.
Rubin et al. in \citep{rubin2007process} addressed the problem of engineering processes that are not well documented and are usually unstructured. They provided a bridge from Kindler et al.'s approach to ProM \citep{van2005prom} in order to mine different process perspectives, such as performance social network analyses. %but not from a project point of view.
Rubin et al. \citep{rubin2014agile} applied process mining to the touristic industry and obtained user processes from web client logs pursuing the goal of improving the software system by analyzing the underlying process.
Poncin et al. \citep{Poncin2011a} developed the FRASR framework for preprocessing software repositories to transform the VCS data to logs that conform to the process mining event log meta model~\citep{van2005meta} as utilized in ProM \citep{van2005prom}.
However, these approaches disregard the single-instance nature of project-oriented business processes and treat them as procedures that can be repeated over time.

The second category of related work focuses on the visualization of VCS data %by defining measurements such as things that can be plotted into a chart.
for different purposes.
%There are two main lines of how this problem was approached in literature.
%\paragraph*{visualization perspective} defining measurements such as things that can be plotted into a chart. But, they haven't any explicit notion of work structure.
%\paragraph*{preprocessing} - VCS data is transformed in order to comply to the process mining meta model from van Dongen and van der Aalst \citep{van2005meta}. This means that they are oriented to discover "classical" processing (i.e. repeated many times in the logs).
Several approaches study the interaction among developers over time from a visualization point of view. For instance, Ogawa and Ma \citep{ogawa2010software} drew storyline pathways to show the story of each developer's contribution. Other approaches analyze and visualize VCS data at file level in order to discover file version evolution. Voinea and Telea~\citep{voinea_multiscale_2006} introduced an interactive navigation method to surf file version evolution as well as two methods to cluster versions of the same file in an abstraction layer. Wu et al.~\citep{jingwei_evolution_2004} also visualized the evolutions of entire projects at file level, emphasizing the evolution moments.
Finally, several approaches study change prediction with the aim of discovering prediction patterns that can help in the process of software development~\citep{zimmermann_mining_2004,ying_predicting_2004}.
%Feld et al. \citep{feldt2013supporting} used software project data to \emph{predict} potential problems during development. Heatmaps, used to visualize source code changes in a two dimensional space, showed to be easily readable also by non experts.
The approaches mentioned in this category as well as others that apply similar techniques~\citep{feldt2013supporting,kagdi_mining_2006,dambros_flexible_2008} focus on studying software evolution from different standpoints. However, the goal pursued differs in all cases from our goal in that they are not interested in discovering projects tasks out of the log data, and hence, they lack an explicit notion of work structure that we need to consider for our purpose.

Our approach combines ideas from both areas, as we aim at identifying tasks like in the approaches that rely on process mining, but we must cluster the data in an appropriate way, for which techniques developed in the approaches that pursue visualization may be adapted or extended.

%----------------
%
%Then explain that, in our work, we don't only want to see what's happening. We want also compliance (which means: did things happen in the right way (as they were allowed to)? Compliance can be provided by Process Mining algorithms... but we cannot apply process mining techniques directly (because they need logs structured in different ways - not svn/git logs). Thus, we need to adopt this strategy. 